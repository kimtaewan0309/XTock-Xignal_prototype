import pandas as pd
import re
import json
from pathlib import Path

# =========================================================
# 1) ì„¤ì •
# =========================================================

# ê´‘ê³ /ìŠ¤íŒ¸ í‚¤ì›Œë“œ
AD_KEYWORDS = [
    "join now", "free", "subscribe", "premium", "alert",
    "breaking picks", "100% gain", "dm us", "signals",
    "our picks", "bonus", "get rich", "indicator", "ğŸ’°"
]

URL_REGEX = r"http[s]?://\S+"
TICKER_REGEX = r"\$[A-Za-z]{1,5}\b"

MIN_LEN = 15  # body ê¸¸ì´ ê¸°ì¤€


# =========================================================
# 2) í—¬í¼ í•¨ìˆ˜: ì „ì²˜ë¦¬ í•„í„°
# =========================================================

def is_spam_or_ad(text: str) -> bool:
    """ê´‘ê³ ì„± ë¬¸êµ¬ í¬í•¨ ì—¬ë¶€"""
    t = text.lower()
    return any(k in t for k in AD_KEYWORDS)


def ticker_only_or_list(text: str) -> bool:
    """í‹°ì»¤ë§Œ ë‚˜ì—´í•œ ë¬¸ì¥ ì—¬ë¶€"""
    tokens = text.strip().split()
    if len(tokens) <= 6:  # ì§€ë‚˜ì¹œ ì œê±° ë°©ì§€ìš©
        tickers = re.findall(TICKER_REGEX, text)
        non_tickers = [tok for tok in tokens if not tok.startswith("$")]

        # í‹°ì»¤ ë¹„ìœ¨ì´ ê³¼ë„í•˜ê²Œ ë†’ì€ ê²½ìš° ì œê±°
        if len(tokens) > 0 and len(tickers) / len(tokens) >= 0.7:
            return True

        # í…ìŠ¤íŠ¸ ì „ì²´ê°€ ticker-onlyë©´ ì œê±°
        if len(non_tickers) == 0:
            return True

    return False


def too_much_url(text: str) -> bool:
    """ë¬¸ì¥ ë‚´ URL ë¹„ìœ¨ì´ ê³¼ë‹¤í•œ ê²½ìš°"""
    urls = re.findall(URL_REGEX, text)
    if not urls:
        return False

    url_len = sum(len(u) for u in urls)
    if url_len / max(len(text), 1) > 0.5:  # 50% ì´ìƒ URL
        return True

    return False


def clean_text(text: str) -> str:
    """ê¸°ë³¸ í…ìŠ¤íŠ¸ ì •ë¦¬"""
    return text.replace("\n", " ").strip()


# =========================================================
# 3) Tweet.csv + Company_Tweet.csv ì „ì²˜ë¦¬
# =========================================================

def preprocess_joined_tweets(tweet_csv, company_csv, sp500_tickers):
    print("[INFO] Loading Tweet.csv and Company_Tweet.csv...")

    df_tweet = pd.read_csv(tweet_csv)
    df_company = pd.read_csv(company_csv)

    # inner join (ê³µí†µ tweet_idë§Œ ìœ ì§€)
    df_merge = df_company.merge(df_tweet, on="tweet_id", how="inner")
    df_merge = df_merge.rename(columns={"body": "text", "ticker_symbol": "ticker"})
    df_merge["text"] = df_merge["text"].astype(str)

    print("[INFO] Joined rows:", len(df_merge))

    # groupby â†’ tweet_id ê¸°ì¤€ multi-label êµ¬ì„±
    grouped = df_merge.groupby("tweet_id").agg({
        "text": "first",
        "ticker": list
    }).reset_index()

    print("[INFO] Grouped unique tweets:", len(grouped))

    cleaned_rows = []

    for _, row in grouped.iterrows():
        text = clean_text(row["text"])
        tickers = [t for t in row["ticker"] if t in sp500_tickers]

        # S&P500 í‹°ì»¤ ì—†ëŠ” row ì œê±°
        if not tickers:
            continue

        # ì‚­ì œ ì¡°ê±´ ì ìš©
        if len(text) < MIN_LEN:
            continue
        if is_spam_or_ad(text):
            continue
        if ticker_only_or_list(text):
            continue
        if too_much_url(text):
            continue

        cleaned_rows.append({
            "doc_id": f"tweet_{row['tweet_id']}",
            "source": "twitter_join",
            "description": text,
            "sp500_labels": list(set(tickers))  # ì¤‘ë³µ ì œê±°
        })

    print("[INFO] Cleaned join tweets:", len(cleaned_rows))
    return cleaned_rows


# =========================================================
# 4) stock_tweets.csv ì „ì²˜ë¦¬
# =========================================================

def preprocess_stock_tweets(stock_csv, sp500_tickers):
    print("[INFO] Loading stock_tweets.csv...")

    df = pd.read_csv(stock_csv)
    df = df.rename(columns={"Tweet": "text", "Stock Name": "ticker"})
    df["text"] = df["text"].astype(str)

    cleaned_rows = []

    for idx, row in df.iterrows():
        text = clean_text(row["text"])
        ticker = row["ticker"]

        if ticker not in sp500_tickers:
            continue

        # ì‚­ì œ ì¡°ê±´
        if len(text) < MIN_LEN:
            continue
        if is_spam_or_ad(text):
            continue
        if ticker_only_or_list(text):
            continue
        if too_much_url(text):
            continue

        cleaned_rows.append({
            "doc_id": f"stock_{idx}",
            "source": "stock_tweet",
            "description": text,
            "sp500_labels": [ticker]
        })

    print("[INFO] Cleaned stock tweets:", len(cleaned_rows))
    return cleaned_rows


# =========================================================
# 5) ì‹¤í–‰ ë° ì €ì¥
# =========================================================

if __name__ == "__main__":
    print("====== Tweet Preprocessing START ======")

    # S&P500 í‹°ì»¤ ëª©ë¡ ë¡œë“œ
    sp500_path = Path("csv/sp500_list.csv")
    if sp500_path.exists():
        df_sp = pd.read_csv(sp500_path)
        sp500_tickers = set(df_sp["ticker"].tolist())
    else:
        print("[WARN] sp500.csvê°€ ì—†ì–´ ëª¨ë“  ticker í—ˆìš©ìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
        sp500_tickers = None

    # ê²½ë¡œ ì„¤ì •
    tweet_csv = "csv/Tweet.csv"
    company_csv = "csv/Company_Tweet.csv"
    stock_csv = "csv/stock_tweets.csv"

    joined_clean = preprocess_joined_tweets(tweet_csv, company_csv, sp500_tickers)
    stock_clean = preprocess_stock_tweets(stock_csv, sp500_tickers)

    # ì €ì¥
    out1 = "preprocessed_joined_tweets.json"
    out2 = "preprocessed_stock_tweets.json"

    with open(out1, "w") as f:
        json.dump(joined_clean, f, indent=2)

    with open(out2, "w") as f:
        json.dump(stock_clean, f, indent=2)

    print("[INFO] Saved:", out1, out2)
    print("====== Preprocessing DONE ======")
